#
# This file is generated by Chef
# Do not edit, changes will be overwritten
#
LoadPlugin "<%= @name %>"

<% if not @config.empty? %>
<Plugin "<%= @name %>">

  ########################################################################
  # Customized plugins written for DonorsChoose.org
  ########################################################################

  # Good places to look for monitoring queries:
  # (1) Munin core plugins
  #     http://munin-monitoring.org/browser/munin?order=name#plugins/node.d
  #     Magnus Hagander
  # (2) The Bucardo project's check_postgres Nagios plugin
  #     https://github.com/bucardo/check_postgres
  #     Greg Sabino Mullane
  #
  # The Result mapping doesn't use ValuesFrom as the time series name.
  # Instead, it uses Type-InstancePrefix, or just Type if no InstancePrefix.
  # Basically, InstancePrefix *must* be provided.
  #
  # See /opt/collectd/share/collectd/types.db for all pre-defined Type
  # names. Use a GAUGE Type to report a number as is.
  # 
  # Use a COUNTER or DERIVE Type if the number should be stored in graphite
  # as a per-second rate. A COUNTER is a continuously incrementing counter
  # with legitimate overflow wrap detection at the 32bit or 64bit border.
  # A counter reset (as opposed to overflow wrap) should be detected, too.
  # A DERIVE with min=0 is similar, without the overflow check, and would
  # be something that fluctuates like a queue length.

  <Query activity>
        # This is a beefed up "backends" query, breaking out counts of
        # transactions in 3 states: running, blocked, idle-in-transaction.
        Statement "SELECT count(*) as num_backends, \
                sum(running) as num_running, \
                sum(blocked) as num_blocked, \
                sum(iit) as num_iit, \
                sum(idle) as num_idle, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    CASE WHEN running = 1 THEN query_start ELSE null END \
                  )))::bigint,0) as max_running_ms, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    CASE WHEN blocked = 1 THEN query_start ELSE null END \
                  )))::bigint,0) as max_blocked_ms, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    CASE WHEN iit = 1 THEN state_change ELSE null END \
                  )))::bigint,0) as max_iit_ms, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    xact_start \
                  )))::bigint,0) as max_xact_ms, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    backend_start \
                  )))::bigint,0) as max_backend_ms, \
                COALESCE(ROUND(1000*EXTRACT(epoch FROM now()-min( \
                    CASE WHEN idle = 1 THEN state_change ELSE null END \
                  )))::bigint,0) as max_idle_ms \
                FROM ( \
                  SELECT CASE WHEN state = 'idle' \
                              THEN 1 ELSE 0 END AS idle, \
                         CASE WHEN state LIKE 'idleintransaction%' \
                              THEN 1 ELSE 0 END AS iit, \
                         CASE WHEN waiting \
                              THEN 1 ELSE 0 END AS blocked, \
                         CASE WHEN NOT waiting AND state NOT LIKE 'idle%' \
                              THEN 1 ELSE 0 END AS running, \
                         CASE WHEN query_start < now() THEN query_start ELSE now() END AS query_start, \
                         CASE WHEN state_change < now() THEN state_change ELSE now() END AS state_change, \
                         CASE WHEN xact_start < now() THEN xact_start ELSE now() END AS xact_start, \
                         CASE WHEN backend_start < now() THEN backend_start ELSE now() END AS backend_start \
                  FROM pg_stat_activity \
                  WHERE datname = $1 \
                ) AS activity_summary;"

        Param database

        # The "current-count-*" breaks out 4 non-overlapping counts
        # of the current state of all established connections. An
        # application with database connection pooling probably keeps
        # most established connections idle in the pool (idle-notinuse).
        # Current transactions are covered by the first 3 states:
        <Result>
                # current-count-active in query execution.
                Type "current"
                InstancePrefix "count-active"
                ValuesFrom "num_running"
        </Result>
        <Result>
                # current-count-blocked waiting for a lock needed for query execution.
                Type "current"
                InstancePrefix "count-blocked"
                ValuesFrom "num_blocked"
        </Result>
        <Result>
                # current-count-idleinxact (idle in transaction) means the application
                # is doing anything other than query execution while still
                # holding a database transaction open.
                Type "current"
                InstancePrefix "count-idleinxact"
                ValuesFrom "num_iit"
        </Result>
        <Result>
                # current-count-notinuse (idle in pool).
                Type "current"
                InstancePrefix "count-notinuse"
                ValuesFrom "num_idle"
        </Result>

        # The "current-longest-*" shows the age (in ms) of the longest
        # running transaction or activity in a transaction. Transactions
        # switch between activities in the first 3 states:
        <Result>
                # current-longest-active -- longest running query
                Type "current"
                InstancePrefix "longest-active"
                ValuesFrom "max_running_ms"
        </Result>
        <Result>
                # current-longest-blocked -- longest waiting for lock
                Type "current"
                InstancePrefix "longest-blocked"
                ValuesFrom "max_blocked_ms"
        </Result>
        <Result>
                # current-longest-idleinxact -- longest idle in transaction
                Type "current"
                InstancePrefix "longest-idleinxact"
                ValuesFrom "max_iit_ms"
        </Result>
        <Result>
                # current-longest-xact -- longest transaction, overall
                Type "current"
                InstancePrefix "longest-xact"
                ValuesFrom "max_xact_ms"
        </Result>

        # The "current-oldest-*" shows age (in ms) of connection states
        # that are unrelated to transactions:
        <Result>
                # current-oldest-connection -- oldest established login
                Type "current"
                InstancePrefix "oldest-connection"
                ValuesFrom "max_backend_ms"
        </Result>
        <Result>
                # current-oldest-notinuse -- oldest connection idle in pool
                Type "current"
                InstancePrefix "oldest-notinuse"
                ValuesFrom "max_idle_ms"
        </Result>
  </Query>

  <Query locks>
        # PostgreSQL lock contention
        # http://grokbase.com/t/postgresql/pgsql-performance/096cna5xgr/what-server-stats-to-track-monitor#20090627b5h4vzhuumzpcqsmoxch6fawea
        Statement "SELECT coalesce(sum(CASE WHEN granted THEN 1 ELSE 0 END), 0) AS granted, \
                coalesce(sum(CASE WHEN NOT granted THEN 1 ELSE 0 END), 0) AS waiting \
                FROM pg_locks \
                WHERE pid <> pg_backend_pid() \
                AND database=(SELECT oid FROM pg_database WHERE datname=$1);"

        Param database

        <Result>
                Type "gauge"
                InstancePrefix "locks-granted"
                ValuesFrom "granted"
        </Result>
        <Result>
                Type "gauge"
                InstancePrefix "locks-waiting"
                ValuesFrom "waiting"
        </Result>
  </Query>

  <Query xlog>
        # PostgreSQL transaction log: Log segments
        # http://munin-monitoring.org/browser/munin/plugins/node.d/postgres_xlog.in
        Statement "SELECT count(*) AS segments \
                FROM pg_ls_dir('pg_xlog') t(fn) \
                WHERE fn ~ '^[0-9A-Z]{24}\$';"

        <Result>
                Type "files"
                InstancePrefix "log-segments"
                ValuesFrom "segments"
        </Result>
  </Query>

  <Query bgwriter>
        # PostgreSQL bgwriter bgwriter buffer statistics
        # http://munin-monitoring.org/browser/munin/plugins/node.d/postgres_bgwriter.in
        # PostgreSQL checkpoints 
        # http://munin-monitoring.org/browser/munin/plugins/node.d/postgres_checkpoints.in

        # I'm picking up all metrics.
        # (Multiplied by 10 so the derive conversion to "per second" over the
        # 10 second window computes the increment -- this is a continuous count)
        Statement "SELECT (10*checkpoints_timed)::bigint as checkpoints_timed, \
                (10*checkpoints_req)::bigint as checkpoints_req, \
                (10*checkpoint_write_time)::bigint as checkpoint_write_time, \
                (10*checkpoint_sync_time)::bigint as checkpoint_sync_time, \
                (10*buffers_checkpoint)::bigint as buffers_checkpoint, \
                (10*buffers_clean)::bigint as buffers_clean, \
                (10*maxwritten_clean)::bigint as maxwritten_clean, \
                (10*buffers_backend)::bigint as buffers_backend, \
                (10*buffers_backend_fsync)::bigint as buffers_backend_fsync, \
                (10*buffers_alloc)::bigint as buffers_alloc \
                FROM pg_stat_bgwriter;"

        # See http://www.westnet.com/~gsmith/content/postgresql/chkp-bgw-83.htm for some explanation.

        # Time spent in checkpoints (_timed or _req not distinguished):
        <Result>
                # Total amount of time that has been spent in the portion of checkpoint
                # processing where files are written to disk, in milliseconds.
                # (One lump sum updated at checkpoint completion.)
                Type "total_time_in_ms"
                InstancePrefix "checkpoint_write"
                ValuesFrom "checkpoint_write_time"
        </Result>
        <Result>
                # Total amount of time that has been spent in the portion of checkpoint
                # processing where files are synchronized to disk, in milliseconds.
                # (Multiplied by 10 so the derive conversion to "per second" over the
                # 10 second window computes the increment -- this is a continuous count)
                # (Seems to update as each sync completes.)
                Type "total_time_in_ms"
                InstancePrefix "checkpoint_sync"
                ValuesFrom "checkpoint_sync_time"
        </Result>

        # There are two ways that checkpoints are triggered:
        <Result>
                # Checkpoints started by timeout
                # (I.e., more than checkpoint_timeout seconds have passed since the last checkpoint)
                Type "operations"
                InstancePrefix "checkpoints_timed"
                ValuesFrom "checkpoints_timed"
        </Result>
        <Result>
                # Checkpoints started by request/because required (which
                # normally only happens when you've filled
                # checkpoint_segments worth of WAL).
                # (I.e., when checkpoint_segments worth of WAL files have been written)
                Type "operations"
                InstancePrefix "checkpoints_req"
                ValuesFrom "checkpoints_req"
        </Result>

        # The LRU bgwriter can write shared buffers out to OS to prepare
        # buffers that we expect will be needed for allocations in the
        # near future.
        # See some LRU bgwriter algorithm explainations by Greg Smith at:
        # http://www.postgresql.org/message-id/alpine.GSO.2.01.0908130343590.13251@westnet.com
        # http://www.postgresql.org/message-id/4B7C7A5A.60807@2ndquadrant.com
        # http://www.westnet.com/~gsmith/content/postgresql/chkp-bgw-83.htm
        <Result>
                # Whether background bgwriter stopped a cleaning scan because it had written too many buffers
                Type "operations"
                InstancePrefix "maxwritten_clean"
                ValuesFrom "maxwritten_clean"
        </Result>

        # From Greg Smith, "PostgreSQL 9.0 High Performance", pg. 114:
        # Once a block has been made dirty in shared memory, there are
        # three possible ways it can be written back to the database,
        # each tracked by a counter in pg_stat_bgwriter:
        #   *  buffers_checkpoint: Clearing all of the dirty blocks
        #      included as part of a checkpoint write are accounted for
        #      here. This value jumps all at once when the checkpoint
        #      finishes, which makes monitoring this an alternate way to
        #      figure out when checkpoints finished in addition to the
        #      log files.
        #  *   buffers_backend: A backend (any process besides the
        #      background writer that also handles checkpoints) tried to
        #      allocate a buffer, and the one it was given to use was
        #      dirty. In that case, the backend must write the dirty
        #      block out itself before it can use the buffer page.
        #  *   buffers_clean: The type of backend write described in the
        #      last few steps is making some other process stall for a
        #      moment while it writes out dirty data. To keep that from
        #      happening as often, the background writer process scans
        #      forward looking for blocks that might be allocated in the
        #      near future that are dirty and that have a low usage
        #      count (alternatively called the Least Recently Used or
        #      LRU blocks). When it finds them, it writes some of them
        #      out pre-emptively, based on historical allocation rates.
        #      Should they get dirty again before they're reallocated to
        #      a backend, that effort was wasted. The usage count and
        #      dynamically tuned approach used by the background writer
        #      make that unlikely.
        # Generally, the best type of write here is the one done by a
        # checkpoint. In each of the other cases, it's possible this
        # block will get dirty again before the next checkpoint, which
        # makes the earlier write a waste of resources.
        <Result>
                # Buffers written when performing a checkpoint
                Type "pg_blks"   # value:DERIVE:0:U
                InstancePrefix "bgwriter_checkpoint"
                ValuesFrom "buffers_checkpoint"
        </Result>
        <Result>
                # Buffers cleaned by background bgwriter runs (time passing)
                Type "pg_blks"   # value:DERIVE:0:U
                InstancePrefix "bgwriter_clean"
                ValuesFrom "buffers_clean"
        </Result>
        <Result>
                # Buffers written by backends (probably the client itself)
                # and not the bgwriter in order to make space for the new
                # allocation (this is a subset of buffers_alloc).
                # This number represents the behavior the LRU bgwriter is
                # trying to prevent--backends having to clean their own
                # buffers up.
                Type "pg_blks"   # value:DERIVE:0:U
                InstancePrefix "bgwriter_backend"
                ValuesFrom "buffers_backend"
        </Result>
        <Result>
                # Buffers written and fsynced by backends (a subset of buffers_backend).
                # This happens when the bgwriter fsync request queue is
                # full, and is generally detrimental to performance, so
                # it's good to know when it's happening. Normally the
                # background writer handles those even when the backend
                # does its own write.
                Type "pg_blks"   # value:DERIVE:0:U
                InstancePrefix "bgwriter_backend_fsync"
                ValuesFrom "buffers_backend_fsync"
        </Result>

        # This is buffers being read from the FS buffer cache (or disk itself).
        <Result>
                # Buffers allocated globally to hold database pages.
                Type "pg_blks"   # value:DERIVE:0:U
                InstancePrefix "bgwriter_alloc"
                ValuesFrom "buffers_alloc"
        </Result>
  </Query>

  <Query config_settings>
        Statement "SELECT name, \
                CASE WHEN vartype = 'bool' AND setting = 'on' \
                     THEN 1 \
                     WHEN vartype = 'bool' AND setting = 'off' \
                     THEN 0 \
                     ELSE setting::numeric \
                 END AS value \
                FROM pg_settings \
                WHERE vartype IN ('integer', 'real', 'bool') \
                AND name IN ( \
                  'block_size', \
                  'server_version_num', \
                  'wal_block_size', \
                  'wal_segment_size', \
                  'cursor_tuple_fraction', \
                  'default_statistics_target', \
                  'from_collapse_limit', \
                  'join_collapse_limit', \
                  'autovacuum', \
                  'autovacuum_analyze_scale_factor', \
                  'autovacuum_analyze_threshold', \
                  'autovacuum_freeze_max_age', \
                  'autovacuum_max_workers', \
                  'autovacuum_naptime', \
                  'autovacuum_vacuum_cost_delay', \
                  'autovacuum_vacuum_cost_limit', \
                  'autovacuum_vacuum_scale_factor', \
                  'autovacuum_vacuum_threshold', \
                  'vacuum_freeze_min_age', \
                  'vacuum_freeze_table_age', \
                  'max_connections', \
                  'cpu_index_tuple_cost', \
                  'cpu_operator_cost', \
                  'cpu_tuple_cost', \
                  'effective_cache_size', \
                  'random_page_cost', \
                  'seq_page_cost', \
                  'effective_io_concurrency', \
                  'bgwriter_delay', \
                  'bgwriter_lru_maxpages', \
                  'bgwriter_lru_multiplier', \
                  'vacuum_cost_delay', \
                  'vacuum_cost_limit', \
                  'vacuum_cost_page_dirty', \
                  'vacuum_cost_page_hit', \
                  'vacuum_cost_page_miss', \
                  'maintenance_work_mem', \
                  'shared_buffers', \
                  'work_mem', \
                  'checkpoint_completion_target', \
                  'checkpoint_segments', \
                  'checkpoint_timeout', \
                  'commit_delay', \
                  'commit_siblings', \
                  'fsync', \
                  'full_page_writes', \
                  'wal_buffers', \
                  'wal_writer_delay' \
                );"

    <Result>
      Type "current" 
      InstancePrefix "setting"
      InstancesFrom "name"
      ValuesFrom "value" 
    </Result>
  </Query>

  <Query streaming_replication_uptime>
       # PostgreSQL pg_stat_replication
       Statement "SELECT name, value
        FROM (
          SELECT CASE WHEN num = 1 THEN 'hot_standby'
                      WHEN num = 2 THEN 'no_standby'
                      ELSE null
                      END AS name,
                 CASE WHEN num = 1
                      THEN EXTRACT(EPOCH FROM age(now(),
                             oldest_live_streaming.backend_start
                           ))/(24*60*60)
                      WHEN num = 2 
                      THEN CASE WHEN oldest_live_streaming.backend_start IS NULL
                                THEN EXTRACT(EPOCH FROM age(now(),
                                       newest_repmgrd.last_monitor_time
                                     ))/(24*60*60)
                                ELSE NULL
                                END
                      ELSE null
                      END AS value
          FROM (SELECT * from generate_series(1, 2) AS num) AS field
          JOIN
            (SELECT max(last_monitor_time) AS last_monitor_time
               FROM repmgr_data.repl_monitor
            ) newest_repmgrd ON 1=1
          LEFT JOIN
            (SELECT min(backend_start) AS backend_start
               FROM pg_stat_replication
            ) oldest_live_streaming
          ON 1=1
          WHERE NOT pg_is_in_recovery()    
        ) stats
        WHERE value IS NOT NULL;"

       <Result> # repmgr.uptime-streaming-hot_standby
                # repmgr.uptime-streaming-no_standby
           Type "uptime"
           InstancePrefix "streaming"
           InstancesFrom "name"
           ValuesFrom "value"
       </Result>
  </Query>

  <Query repmgrd_not_running>
       # Complains (non-null) if backend_start more recent than last_monitor_time,
       # which assumes repmgrd never exits while replication continues.
       Statement "SELECT 'down_on_' || cluster || standby_node AS name,
                 CASE WHEN backend_start > last_monitor_time
                      THEN EXTRACT(EPOCH FROM age(now(), last_monitor_time))/(24*60*60)
                      ELSE NULL
                      END AS value
            FROM (SELECT repl_nodes.cluster,
                         repl_status.standby_node,
                         pg_stat_replication.backend_start,
                         max(last_monitor_time) as last_monitor_time
                    FROM pg_stat_replication -- Only shows rows on master
                    JOIN repmgr_data.repl_nodes 
                      ON repl_nodes.conninfo LIKE 'host=' || host(client_addr) || ' %'
                    JOIN repmgr_data.repl_status 
                      ON repl_status.standby_node = repl_nodes.id 
                   GROUP BY backend_start, cluster, standby_node
            ) live_streaming
          WHERE backend_start > last_monitor_time
          ORDER BY standby_node;"

        <Result> # repmgr.uptime-repmgrd-down_on_data*
            Type "uptime"
            InstancePrefix "repmgrd"
            InstancesFrom "name"
            ValuesFrom "value"
        </Result>
  </Query>

  <Query streaming_replication_lags>
       # PostgreSQL pg_stat_replication
       Statement "SELECT CASE WHEN num = 1 THEN 'send_to_'
                      WHEN num = 2 THEN 'recv_at_'
                      WHEN num = 3 THEN 'flush_at_'
                      WHEN num = 4 THEN 'replay_at_'
                      ELSE null
                      END || cluster || standby_node AS name,
                 CASE WHEN num = 1 THEN sent_lag::bigint
                      WHEN num = 2 THEN received_lag::bigint
                      WHEN num = 3 THEN flush_lag::bigint
                      WHEN num = 4 THEN replay_lag::bigint
                      ELSE null
                      END AS value
            FROM (SELECT * from generate_series(1, 4) AS num) AS step
            JOIN (SELECT repl_nodes.cluster,
                         repl_status.standby_node,
                         pg_xlog_location_diff(pg_current_xlog_location(), sent_location)
                           AS sent_lag,
                         pg_xlog_location_diff(sent_location, write_location)
                           AS received_lag,
                         pg_xlog_location_diff(write_location, flush_location)
                           AS flush_lag,
                         pg_xlog_location_diff(flush_location, replay_location)
                           AS replay_lag
                    FROM pg_stat_replication -- Only shows rows on master
                    JOIN repmgr_data.repl_nodes 
                      ON repl_nodes.conninfo LIKE 'host=' || host(client_addr) || ' %'
                    JOIN repmgr_data.repl_status 
                      ON repl_status.standby_node = repl_nodes.id 
            ) live_streaming on 1=1
          ORDER BY standby_node, step.num;"

        <Result> # repmgr.bytes-replication_lag-send_to_data*
            Type "bytes"
            InstancePrefix "replication_lag"
            InstancesFrom "name"
            ValuesFrom "value"
        </Result>
  </Query>

  ########################################################################
  # Configuration of database connections and queries to be executed.
  ########################################################################
  # Database connections are configured by name-value attributes in
  # * /var/chef-solo/roles/*.rb files for cluster-wide settings, which
  #   are identical for each virtual machine configuration.
  #   (Note this is not the *-blueprint.rb files for appliance builds).
  # * /var/chef-solo/nodes/*.json files for schema-specific settings.

  <% @config.sort.map do |dbname, params|
       if dbname == "postgres" %>
  # Cluster-wide monitoring
  <Database "<%= dbname %>">
    <% params.sort.map do |key, value|
       if key != "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>

    <% params.sort.map do |key, value|
       if key == "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>
  </Database>
  <% end end %>

  <% @config.sort.map do |dbname, params|
       if dbname == "repmgr" %>
  # Replication monitoring
  <Database "<%= dbname %>">
    <% params.sort.map do |key, value|
       if key != "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>

    <% params.sort.map do |key, value|
       if key == "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>
  </Database>
  <% end end %>

  <% @config.sort.map do |dbname, params|
       if dbname != "postgres" and dbname != "repmgr" %>
  # Schema-specific monitoring for "<%= dbname %>"
  <Database "<%= dbname %>">
    <% params.sort.map do |key, value|
       if key != "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>

    <% params.sort.map do |key, value|
       if key == "Query"
         if value.is_a? Array
           value.each do |subvalue| %>
    <%= key %> <%= collectd_option(subvalue) %>
    <%   end else %>
    <%= key %> <%= collectd_option(value) %>
    <%   end end end %>
  </Database>

  <% end end %>
</Plugin>

<% end %>
